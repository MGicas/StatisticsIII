{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2f82aa7-dda0-4545-8375-e0c273f94747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import tarfile\n",
    "import zipfile\n",
    "from scipy.io import loadmat\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve, precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d12fb08-12ef-45c3-815e-41da9b7e17dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/train.csv'"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"./data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d07b74e-db4c-41be-8a3c-7dc125efbcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_palabras = 0\n",
    "\n",
    "for fila in train_data.iterrows():\n",
    "    for columna in train_data.columns:\n",
    "        celda = str(fila[1][columna])\n",
    "        palabras = celda.split()\n",
    "        total_palabras += len(palabras)\n",
    "\n",
    "print(\"Total de palabras en el dataset:\", total_palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c476b7ba-dca5-4e48-b6c6-e94ee1357d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener una lista de todos los títulos\n",
    "titulos = train_data['titulo'].tolist()\n",
    "\n",
    "# Crear una matriz (DataFrame) con palabras únicas como columnas\n",
    "palabras_unicas = list(set(' '.join(titulos).split()))\n",
    "matriz_frecuencia = pd.DataFrame(0, columns=palabras_unicas, index=titulos)\n",
    "\n",
    "# Bucle para contar la frecuencia de palabras en cada título\n",
    "for titulo in titulos:\n",
    "    palabras = titulo.split()\n",
    "    for palabra in palabras:\n",
    "        matriz_frecuencia.at[titulo, palabra] += 1\n",
    "\n",
    "# Imprimir la matriz de frecuencia\n",
    "print(matriz_frecuencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d353bf32-f8ef-46a3-a539-00523de232d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "todos_titulos = ' '.join(titulos)\n",
    "\n",
    "# Dividir la cadena en palabras\n",
    "palabras = todos_titulos.split()\n",
    "\n",
    "# Contar la frecuencia de cada palabra\n",
    "contador_palabras = Counter(palabras)\n",
    "\n",
    "# Obtener las 10 palabras más repetidas\n",
    "palabras_mas_repetidas = contador_palabras.most_common(10)\n",
    "\n",
    "# Imprimir las 10 palabras más repetidas\n",
    "for palabra, frecuencia in palabras_mas_repetidas:\n",
    "    print(f'Palabra: {palabra}, Frecuencia: {frecuencia}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36298209-2db4-4355-a110-70fb8a9459de",
   "metadata": {},
   "outputs": [],
   "source": [
    "eliminar_palabras = [\"-\", \"de\", \"que\", \"vemos\", \"lo\", \"todo\", \"en\", \"Televidente\", \"la\", \"y\"]\n",
    "\n",
    "# Función para eliminar palabras específicas de un título y reemplazar tildes y \"ñ\"\n",
    "def procesar_titulo(titulo):\n",
    "    # Eliminar palabras específicas\n",
    "    palabras = titulo.split()\n",
    "    palabras_filtradas = [palabra for palabra in palabras if palabra not in eliminar_palabras]\n",
    "    \n",
    "    # Reemplazar tildes y \"ñ\"\n",
    "    titulo_modificado = ' '.join(palabras_filtradas)\n",
    "    titulo_modificado = titulo_modificado.replace('á', 'a').replace('é', 'e').replace('í', 'i').replace('ó', 'o').replace('ú', 'u')\n",
    "    titulo_modificado = titulo_modificado.replace('Á', 'A').replace('É', 'E').replace('Í', 'I').replace('Ó', 'O').replace('Ú', 'U')\n",
    "    titulo_modificado = titulo_modificado.replace('ñ', 'n').replace('Ñ', 'N')\n",
    "    \n",
    "    return titulo_modificado\n",
    "\n",
    "# Aplicar la función para procesar el título a la columna 'titulo'\n",
    "train_data['titulo_procesado'] = train_data['titulo'].apply(procesar_titulo)\n",
    "\n",
    "# Imprimir el DataFrame resultante\n",
    "print(train_data['titulo_procesado'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59605a1-6e60-4e54-ad44-31f0a9f30763",
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table = pd.crosstab(train_data['titulo_procesado'], train_data['categoria'])\n",
    "\n",
    "correlaciones = []\n",
    "\n",
    "# Calcular el coeficiente de correlación de Cramer-V para cada título y categoría\n",
    "for titulo in train_data['titulo_procesado'].unique():\n",
    "    if titulo in contingency_table.index:\n",
    "        observed = contingency_table.loc[titulo].to_numpy().astype(np.float64)  # Asegurarse de que observed sea de tipo float64\n",
    "        observed += 0.5  # Sumar 0.5\n",
    "        chi2, _, _, _ = chi2_contingency([observed])\n",
    "        n = observed.sum()\n",
    "        phi2 = chi2 / n\n",
    "        phi2corr = max(0, phi2 - 1 / n)\n",
    "        cramer_v = np.sqrt(phi2corr)\n",
    "    \n",
    "        correlaciones.append((titulo, cramer_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb638b-04ce-43fa-ac7c-953f3f5cef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "# Ajustar y transformar el título procesado para obtener la matriz de características\n",
    "matriz_caracteristicas = vectorizer.fit_transform(train_data['titulo_procesado'])\n",
    "\n",
    "# Convertir la matriz de características en un DataFrame de pandas\n",
    "matriz_caracteristicas_df = pd.DataFrame(matriz_caracteristicas.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Imprimir la matriz de características\n",
    "print(matriz_caracteristicas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17191864-fbef-4a03-99a1-b9b7b9940e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "titulos = train_data['titulo_procesado'].tolist()\n",
    "matriz_frecuencia = pd.DataFrame(0, columns=palabras_unicas, index=titulos)\n",
    "\n",
    "for titulo in titulos:\n",
    "    palabras = titulo.split()\n",
    "    total_palabras = len(palabras)\n",
    "    \n",
    "    for palabra in palabras:\n",
    "        frecuencia = palabras.count(palabra)\n",
    "        frecuencia_relativa = frecuencia / total_palabras\n",
    "        matriz_frecuencia.at[titulo, palabra] = frecuencia_relativa\n",
    "\n",
    "# Imprimir la matriz de frecuencia relativa\n",
    "print(matriz_frecuencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8ba79d-80be-4499-9353-0d9e6c564edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el número de documentos (títulos) que contienen cada palabra\n",
    "def calcular_numero_documentos_con_palabra(data, columna_texto, palabras_unicas):\n",
    "    num_documentos_con_palabra = []\n",
    "    for palabra in palabras_unicas:\n",
    "        num_documentos = sum(data[columna_texto].str.contains(r'\\b{}\\b'.format(palabra), case=False, regex=False))\n",
    "        num_documentos_con_palabra.append(num_documentos)\n",
    "    return num_documentos_con_palabra\n",
    "\n",
    "# Calcular el IDF para cada palabra\n",
    "def calcular_idf(total_documentos, num_documentos_con_palabra):\n",
    "    idf = [np.log(total_documentos / (num + 1)) for num in num_documentos_con_palabra]\n",
    "    return idf\n",
    "\n",
    "palabras_unicas = list(set(' '.join(train_data['titulo_procesado']).split()))\n",
    "\n",
    "# Calcular el número total de documentos (títulos)\n",
    "total_documentos = len(train_data)\n",
    "\n",
    "# Calcular el número de documentos que contienen cada palabra\n",
    "num_documentos_con_palabra = calcular_numero_documentos_con_palabra(train_data, 'titulo', palabras_unicas)\n",
    "\n",
    "# Calcular el IDF para cada palabra\n",
    "idf = calcular_idf(total_documentos, num_documentos_con_palabra)\n",
    "\n",
    "# Crear un DataFrame para mostrar los resultados\n",
    "idf_df = pd.DataFrame({'Palabra': palabras_unicas, 'IDF': idf})\n",
    "\n",
    "# Imprimir el DataFrame de IDF\n",
    "print(idf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff0922-b200-4677-81a0-9c22fa6affdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_df = idf_df[:800]\n",
    "dataframe = pd.DataFrame(idf_df['IDF'])\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3102dc73-7d52-42fd-baf9-54adcf7b6c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_data['categoria']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b7ac03-c3da-476f-96e2-4f06da2daf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(dataframe, y, train_size = 0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d55afe0-9e56-44ee-9563-ed0af891f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataframe.values\n",
    "y = train_data['categoria'].values\n",
    "\n",
    "numero_de_clases = len(np.unique(y))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(x_train.shape[1], 1)))  \n",
    "model.add(Dense(numero_de_clases, activation='softmax'))  \n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4fc6b4-184b-4470-a7fd-00bc56fc8ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carga los datos de entrenamiento y prueba\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['titulo'])\n",
    "y_train = train_data['categoria']\n",
    "\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "best_C = grid_search.best_params_['C']\n",
    "\n",
    "lr = LogisticRegression(C=best_C)\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_data['titulo'])\n",
    "\n",
    "y_pred = lr.predict(X_test_tfidf)\n",
    "\n",
    "results = pd.DataFrame({'ID': test_data['index'], 'categoria': y_pred})\n",
    "\n",
    "results.to_csv('predicciones3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
